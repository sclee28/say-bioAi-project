{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd5bea1",
   "metadata": {},
   "source": [
    "# 1.ì†ì‹¤ í•¨ìˆ˜ ë° ë©”íŠ¸ë¦­ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86011eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ ë° ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ\n",
    "class AdaptiveBraTSLoss(nn.Module):\n",
    "    \"\"\"ì ì‘í˜• BraTS ì†ì‹¤ í•¨ìˆ˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤)\"\"\"\n",
    "    \n",
    "    def __init__(self, ce_weight=1.0, dice_weight=1.0, focal_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        \n",
    "        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n",
    "        self.class_weights = torch.tensor([0.1, 2.0, 1.5, 2.5]).to(device)\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        print(\"ğŸ“Š ì ì‘í˜• ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™” (CE + Dice + Focal)\")\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Cross-Entropy Loss\n",
    "        ce_loss = self.ce_loss(predictions, targets)\n",
    "        \n",
    "        # Multi-Class Dice Loss\n",
    "        dice_loss = self._multiclass_dice_loss(predictions, targets)\n",
    "        \n",
    "        # Focal Loss\n",
    "        focal_loss = self._focal_loss(predictions, targets)\n",
    "        \n",
    "        # ì´ ì†ì‹¤\n",
    "        total_loss = (self.ce_weight * ce_loss +\n",
    "                     self.dice_weight * dice_loss +\n",
    "                     self.focal_weight * focal_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def _multiclass_dice_loss(self, predictions, targets):\n",
    "        \"\"\"ë‹¤ì¤‘ í´ë˜ìŠ¤ Dice ì†ì‹¤\"\"\"\n",
    "        smooth = 1e-6\n",
    "        probs = F.softmax(predictions, dim=1)\n",
    "        \n",
    "        # ì›-í•« ì¸ì½”ë”©\n",
    "        targets_one_hot = F.one_hot(targets.long(), num_classes=predictions.shape[1])\n",
    "        targets_one_hot = targets_one_hot.permute(0, 4, 1, 2, 3).float()\n",
    "        \n",
    "        dice_scores = []\n",
    "        for i in range(predictions.shape[1]):\n",
    "            pred_i = probs[:, i]\n",
    "            target_i = targets_one_hot[:, i]\n",
    "            \n",
    "            intersection = (pred_i * target_i).sum()\n",
    "            union = pred_i.sum() + target_i.sum()\n",
    "            dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "            \n",
    "            class_weight = self.class_weights[i] if i < len(self.class_weights) else 1.0\n",
    "            dice_scores.append(dice * class_weight)\n",
    "        \n",
    "        return 1.0 - torch.stack(dice_scores).mean()\n",
    "    \n",
    "    def _focal_loss(self, predictions, targets, alpha=0.25, gamma=2.0):\n",
    "        \"\"\"Focal Loss\"\"\"\n",
    "        ce_loss = F.cross_entropy(predictions, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™”\n",
    "criterion = AdaptiveBraTSLoss(ce_weight=1.0, dice_weight=2.0, focal_weight=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f5aa6",
   "metadata": {},
   "source": [
    "# 2. ì„±ëŠ¥ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c514dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveMetrics:\n",
    "    \"\"\"í¬ê´„ì  ì„±ëŠ¥ ë©”íŠ¸ë¦­\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4):\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "        self.class_names = ['Background', 'NCR/NET', 'ED', 'ET']\n",
    "        print(\"ğŸ“ í¬ê´„ì  ë©”íŠ¸ë¦­ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"ë©”íŠ¸ë¦­ ì´ˆê¸°í™”\"\"\"\n",
    "        self.all_predictions = []\n",
    "        self.all_targets = []\n",
    "        self.dice_scores = []\n",
    "        self.iou_scores = []\n",
    "        self.processing_times = []\n",
    "    \n",
    "    def update(self, predictions, targets, processing_time=None):\n",
    "        \"\"\"ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\"\"\"\n",
    "        with torch.no_grad():\n",
    "            if predictions.dim() == 5:  # (B, C, H, W, D)\n",
    "                probs = F.softmax(predictions, dim=1)\n",
    "                preds = torch.argmax(probs, dim=1)\n",
    "            else:\n",
    "                preds = predictions\n",
    "            \n",
    "            # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥\n",
    "            preds_np = preds.cpu().numpy().flatten()\n",
    "            targets_np = targets.cpu().numpy().flatten()\n",
    "            \n",
    "            self.all_predictions.extend(preds_np)\n",
    "            self.all_targets.extend(targets_np)\n",
    "            \n",
    "            # ë°°ì¹˜ë³„ Dice ë° IoU ê³„ì‚°\n",
    "            batch_dice = self._calculate_batch_dice(preds, targets)\n",
    "            batch_iou = self._calculate_batch_iou(preds, targets)\n",
    "            \n",
    "            self.dice_scores.extend(batch_dice)\n",
    "            self.iou_scores.extend(batch_iou)\n",
    "            \n",
    "            if processing_time:\n",
    "                self.processing_times.append(processing_time)\n",
    "    \n",
    "    def _calculate_batch_dice(self, preds, targets):\n",
    "        \"\"\"ë°°ì¹˜ë³„ Dice ì ìˆ˜ ê³„ì‚°\"\"\"\n",
    "        batch_dice = []\n",
    "        batch_size = preds.shape[0]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            pred_b = preds[b]\n",
    "            target_b = targets[b]\n",
    "            dice_per_class = []\n",
    "            \n",
    "            for class_id in range(1, self.num_classes):  # ë°°ê²½ ì œì™¸\n",
    "                pred_mask = (pred_b == class_id).float()\n",
    "                target_mask = (target_b == class_id).float()\n",
    "                \n",
    "                intersection = (pred_mask * target_mask).sum()\n",
    "                union = pred_mask.sum() + target_mask.sum()\n",
    "                \n",
    "                if union > 0:\n",
    "                    dice = (2.0 * intersection / union).item()\n",
    "                else:\n",
    "                    dice = 1.0 if intersection == 0 else 0.0\n",
    "                \n",
    "                dice_per_class.append(dice)\n",
    "            \n",
    "            batch_dice.append(np.mean(dice_per_class))\n",
    "        \n",
    "        return batch_dice\n",
    "    \n",
    "    def get_comprehensive_results(self):\n",
    "        \"\"\"í¬ê´„ì  ê²°ê³¼ ë°˜í™˜\"\"\"\n",
    "        if not self.all_predictions or not self.all_targets:\n",
    "            return {'error': 'ê³„ì‚°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'}\n",
    "        \n",
    "        # ê¸°ë³¸ ë¶„ë¥˜ ë©”íŠ¸ë¦­\n",
    "        accuracy = accuracy_score(self.all_targets, self.all_predictions)\n",
    "        precision = precision_score(self.all_targets, self.all_predictions,\n",
    "                                   average='weighted', zero_division=0)\n",
    "        recall = recall_score(self.all_targets, self.all_predictions,\n",
    "                             average='weighted', zero_division=0)\n",
    "        f1 = f1_score(self.all_targets, self.all_predictions,\n",
    "                      average='weighted', zero_division=0)\n",
    "        \n",
    "        # ë¶„í•  ë©”íŠ¸ë¦­\n",
    "        mean_dice = np.mean(self.dice_scores) if self.dice_scores else 0.0\n",
    "        mean_iou = np.mean(self.iou_scores) if self.iou_scores else 0.0\n",
    "        \n",
    "        # ì²˜ë¦¬ ì‹œê°„\n",
    "        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0.0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'dice_score': mean_dice,\n",
    "            'iou_score': mean_iou,\n",
    "            'processing_time': avg_processing_time,\n",
    "            'total_samples': len(self.all_predictions)\n",
    "        }\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ì´ˆê¸°í™”\n",
    "metrics = ComprehensiveMetrics(num_classes=config.num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4528100",
   "metadata": {},
   "source": [
    "# 3. í›ˆë ¨ ì‹œìŠ¤í…œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b75873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ í›ˆë ¨ ì‹œìŠ¤í…œ\n",
    "class Trainer:\n",
    "    \"\"\"ì§€ëŠ¥í˜• í›ˆë ¨ ì‹œìŠ¤í…œ\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # ì˜µí‹°ë§ˆì´ì €\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer,\n",
    "            T_0=5,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # ì†ì‹¤ í•¨ìˆ˜\n",
    "        self.criterion = AdaptiveBraTSLoss()\n",
    "        \n",
    "        # ì„±ëŠ¥ ì¶”ì \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_dice': [], 'val_dice': [],\n",
    "            'learning_rate': [], 'epoch_time': []\n",
    "        }\n",
    "        \n",
    "        self.best_dice = 0.0\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.early_stop_patience = 8\n",
    "        \n",
    "        print(\"ğŸš€ ìŠ¤ë§ˆíŠ¸ í›ˆë ¨ ì‹œìŠ¤í…œ ì´ˆê¸°í™”\")\n",
    "        print(f\" â””â”€â”€ ì˜µí‹°ë§ˆì´ì €: AdamW (lr={config.learning_rate})\")\n",
    "        print(f\" â””â”€â”€ ìŠ¤ì¼€ì¤„ëŸ¬: CosineAnnealingWarmRestarts\")\n",
    "        print(f\" â””â”€â”€ ì¡°ê¸° ì¢…ë£Œ: patience={self.early_stop_patience}\")\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"í›ˆë ¨ ì—í¬í¬\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        running_loss = 0.0\n",
    "        train_metrics = ComprehensiveMetrics(self.config.num_classes)\n",
    "        processed_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f\"í›ˆë ¨ Epoch {epoch}\")\n",
    "        \n",
    "        for batch_idx, (volumes, targets) in enumerate(pbar):\n",
    "            try:\n",
    "                volumes = volumes.to(self.config.device, non_blocking=True)\n",
    "                targets = targets.to(self.config.device, non_blocking=True)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                batch_start_time = time.time()\n",
    "                outputs = self.model(volumes)\n",
    "                processing_time = time.time() - batch_start_time\n",
    "                \n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"âš ï¸ ë°°ì¹˜ {batch_idx}: NaN/Inf ì†ì‹¤ ê°ì§€, ìŠ¤í‚µ\")\n",
    "                    continue\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(),\n",
    "                    max_norm=self.config.gradient_clip_norm\n",
    "                )\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                train_metrics.update(outputs, targets, processing_time)\n",
    "                processed_batches += 1\n",
    "                \n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{loss.item():.4f}',\n",
    "                    'LR': f'{current_lr:.2e}'\n",
    "                })\n",
    "                \n",
    "                if batch_idx % 3 == 0:\n",
    "                    self._cleanup_memory()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë°°ì¹˜ {batch_idx} í›ˆë ¨ ì‹¤íŒ¨: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = running_loss / max(processed_batches, 1)\n",
    "        train_results = train_metrics.get_comprehensive_results()\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        return avg_loss, train_results['dice_score'], epoch_time\n",
    "    \n",
    "    def validate_epoch(self, epoch):\n",
    "        \"\"\"ê²€ì¦ ì—í¬í¬\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        val_metrics = ComprehensiveMetrics(self.config.num_classes)\n",
    "        processed_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.val_loader, desc=f\"ê²€ì¦ Epoch {epoch}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (volumes, targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    volumes = volumes.to(self.config.device, non_blocking=True)\n",
    "                    targets = targets.to(self.config.device, non_blocking=True)\n",
    "                    \n",
    "                    batch_start_time = time.time()\n",
    "                    outputs = self.model(volumes)\n",
    "                    processing_time = time.time() - batch_start_time\n",
    "                    \n",
    "                    loss = self.criterion(outputs, targets)\n",
    "                    \n",
    "                    if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                        running_loss += loss.item()\n",
    "                        val_metrics.update(outputs, targets, processing_time)\n",
    "                        processed_batches += 1\n",
    "                        \n",
    "                    pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        avg_loss = running_loss / max(processed_batches, 1)\n",
    "        val_results = val_metrics.get_comprehensive_results()\n",
    "        \n",
    "        return avg_loss, val_results['dice_score'], val_results\n",
    "    \n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"ì „ì²´ í›ˆë ¨ í”„ë¡œì„¸ìŠ¤\"\"\"\n",
    "        print(f\"ğŸ¯ í›ˆë ¨ ì‹œì‘: {num_epochs} ì—í¬í¬\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\nğŸ“… Epoch {epoch}/{num_epochs}\")\n",
    "            \n",
    "            # í›ˆë ¨\n",
    "            train_loss, train_dice, epoch_time = self.train_epoch(epoch)\n",
    "            \n",
    "            # ê²€ì¦\n",
    "            val_loss, val_dice, val_results = self.validate_epoch(epoch)\n",
    "            \n",
    "            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "            self.scheduler.step()\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_dice'].append(train_dice)\n",
    "            self.history['val_dice'].append(val_dice)\n",
    "            self.history['learning_rate'].append(current_lr)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            \n",
    "            # ê²°ê³¼ ì¶œë ¥\n",
    "            print(f\"í›ˆë ¨ - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}\")\n",
    "            print(f\"ê²€ì¦ - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}\")\n",
    "            print(f\"í•™ìŠµë¥ : {current_lr:.2e}, ì‹œê°„: {epoch_time:.1f}ì´ˆ\")\n",
    "            \n",
    "            # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì²´í¬\n",
    "            is_best = val_dice > self.best_dice\n",
    "            if is_best:\n",
    "                self.best_dice = val_dice\n",
    "                self.patience_counter = 0\n",
    "                self.save_checkpoint(epoch, is_best=True)\n",
    "                print(f\"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥: Dice {val_dice:.4f}\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬\n",
    "            if self.patience_counter >= self.early_stop_patience:\n",
    "                print(f\"â¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {self.early_stop_patience} ì—í¬í¬ ê°œì„  ì—†ìŒ\")\n",
    "                break\n",
    "            \n",
    "            # ì •ê¸° ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "            if epoch % self.config.memory_cleanup_interval == 0:\n",
    "                self._cleanup_memory(full_cleanup=True)\n",
    "                print(\"ğŸ§¹ ì „ì²´ ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰\")\n",
    "        \n",
    "        print(f\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ! ìµœê³  Dice: {self.best_dice:.4f}\")\n",
    "        return self.history\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_dice': self.best_dice,\n",
    "            'history': self.history,\n",
    "            'config_dict': {\n",
    "                'in_channels': self.config.in_channels,\n",
    "                'num_classes': self.config.num_classes,\n",
    "                'base_features': self.config.base_features,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = \"best_model.pth\" if is_best else f\"checkpoint_epoch_{epoch}.pth\"\n",
    "        filepath = os.path.join(self.config.model_save_path, filename)\n",
    "        torch.save(checkpoint, filepath)\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"ğŸ’¾ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: {filepath}\")\n",
    "    \n",
    "    def _cleanup_memory(self, full_cleanup=False):\n",
    "        \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "        if self.config.device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        elif torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if full_cleanup:\n",
    "            gc.collect()\n",
    "\n",
    "# í›ˆë ¨ì ìƒì„±\n",
    "trainer = SmartTrainer(model, train_loader, val_loader, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149d57f",
   "metadata": {},
   "source": [
    "# 4. í›ˆë ¨ ì‹¤í–‰ ë° ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4769500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\n",
    "def monitor_memory():\n",
    "    \"\"\"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        try:\n",
    "            allocated = torch.mps.current_allocated_memory() / 1024**2\n",
    "            reserved = torch.mps.driver_allocated_memory() / 1024**2\n",
    "            print(f\"ğŸ’¾ MPS ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated:.1f}MB, ì˜ˆì•½: {reserved:.1f}MB\")\n",
    "        except:\n",
    "            print(\"ğŸ’¾ MPS ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"ğŸ’¾ MPS ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë¶ˆê°€\")\n",
    "\n",
    "def visualize_training_results(history):\n",
    "    \"\"\"í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”\"\"\"\n",
    "    if not history['train_loss']:\n",
    "        print(\"âš ï¸ í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ğŸ“Š í›ˆë ¨ ê²°ê³¼ ë¶„ì„', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # 1. ì†ì‹¤ í•¨ìˆ˜ ë³€í™”\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='í›ˆë ¨ Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='ê²€ì¦ Loss', linewidth=2)\n",
    "    ax1.set_title('ì†ì‹¤ í•¨ìˆ˜ ë³€í™”')\n",
    "    ax1.set_xlabel('ì—í¬í¬')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Dice ì ìˆ˜ ë³€í™”\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs, history['train_dice'], 'g-', label='í›ˆë ¨ Dice', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_dice'], 'orange', label='ê²€ì¦ Dice', linewidth=2)\n",
    "    ax2.set_title('Dice ì ìˆ˜ ë³€í™”')\n",
    "    ax2.set_xlabel('ì—í¬í¬')\n",
    "    ax2.set_ylabel('Dice Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. í•™ìŠµë¥  ë³€í™”\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.semilogy(epochs, history['learning_rate'], 'purple', linewidth=2)\n",
    "    ax3.set_title('í•™ìŠµë¥  ë³€í™”')\n",
    "    ax3.set_xlabel('ì—í¬í¬')\n",
    "    ax3.set_ylabel('Learning Rate (log scale)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ì—í¬í¬ë³„ ì‹œê°„\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.bar(epochs, history['epoch_time'], color='skyblue', alpha=0.7)\n",
    "    ax4.set_title('ì—í¬í¬ë³„ í›ˆë ¨ ì‹œê°„')\n",
    "    ax4.set_xlabel('ì—í¬í¬')\n",
    "    ax4.set_ylabel('ì‹œê°„ (ì´ˆ)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸš€ BraTS 2021 ê°œì„  ë²„ì „ í›ˆë ¨ ì‹œì‘!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ\n",
    "monitor_memory()\n",
    "\n",
    "try:\n",
    "    # ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "    print(\"ğŸ§ª ëª¨ë¸ êµ¬ì¡° í…ŒìŠ¤íŠ¸...\")\n",
    "    test_volume = torch.randn(1, 4, 64, 64, 64).to(config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_output = model(test_volume)\n",
    "    \n",
    "    print(f\" âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_volume.shape} â†’ {test_output.shape}\")\n",
    "    del test_volume, test_output\n",
    "    \n",
    "    if config.device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    # ì‹¤ì œ í›ˆë ¨ ì‹¤í–‰\n",
    "    print(f\"\\nğŸ¯ í›ˆë ¨ ì‹œì‘ ({config.epochs} ì—í¬í¬)\")\n",
    "    history = trainer.train(num_epochs=config.epochs)\n",
    "    \n",
    "    # ê²°ê³¼ ì‹œê°í™”\n",
    "    print(f\"\\nğŸ“Š í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”\")\n",
    "    visualize_training_results(history)\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "    print(f\"\\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!\")\n",
    "    print(f\" ğŸ† ìµœê³  Dice ì ìˆ˜: {trainer.best_dice:.4f}\")\n",
    "    \n",
    "    if history['train_loss']:\n",
    "        print(f\" ğŸ“‰ ìµœì¢… í›ˆë ¨ Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\" ğŸ“‰ ìµœì¢… ê²€ì¦ Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\" ğŸ“ˆ ìµœì¢… ê²€ì¦ Dice: {history['val_dice'][-1]:.4f}\")\n",
    "        print(f\" â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {sum(history['epoch_time']):.1f}ì´ˆ\")\n",
    "        print(f\" âš¡ í‰ê·  ì—í¬í¬ ì‹œê°„: {np.mean(history['epoch_time']):.1f}ì´ˆ\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    trainer._cleanup_memory(full_cleanup=True)\n",
    "    monitor_memory()\n",
    "    print(\"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "print(f\"\\nğŸ ëª¨ë“  ì‘ì—… ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
